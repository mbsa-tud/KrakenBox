# Runtime params
# =============================================================================

sim_t: 50 #training->2500
sample_t: 0.1

# connection
xavier_eth0: "169.254.108.105" #"129.69.81.73"
simulink_eth0: "192.168.178.46"

run_id: "2020-04-02_19.13.36" 


animation: False # optional:  "animation" "plot_cla"

# path
path_root: '/home/krakenboxtud/Desktop/kraken_box' # 'F:\\code\\py\\pyKraken\\kraken\\kraken-master\\kraken-master'
path_model: '/home/krakenboxtud/Desktop/kraken_box/kraken/models/trained_online'


train_data: '2500npz'  # options:  'online' '2500npz'
load_model: 'online'   # options:  'online' 'offline'

# output is 10 data, which one you want to take as standard/flag/feature? 
# counting from the lost point, 10 means the firt point
take_point: 1  #10...1 


# number of values to evaluate in each batch
batch_size: 70

# number of trailing batches to use in error calculation
window_size: 30




# Error thresholding parameters
# =============================================================================
threshold: {'R2S1':0.3,'R2S2':0.6,'R2S3':0.3}



# LSTM parameters
# =============================================================================
loss: 'mse'
metrics: 'mae'
optimizer: 'adam'
validation_split: 0.2
dropout: 0.3
lstm_batch_size: 64



# maximum number of epochs allowed (if early stopping criteria not met)
epochs: 35

# network architecture [<neurons in hidden layer>, <neurons in hidden layer>]
# Size of input layer not listed - dependent on evr modules and types included (see 'evr_modules' and 'erv_types' above)
layers: [80,80]

# Number of consequetive training iterations to allow without decreasing the val_loss by at least min_delta 
patience: 10
min_delta: 0.0003

# num previous timesteps provided to model to predict future values
l_b: 50

# number of steps ahead to predict
l_f: 1 #10








# Misc
# =============================================================================
mode: 'Training'

# Columns headers for output file
header: ["run_id", "chan_id", "spacecraft", "num_anoms", "anomaly_sequences", "class", "true_positives", 
        "false_positives", "false_negatives", "tp_sequences", "fp_sequences", "gaussian_p-value", "num_values",
        "normalized_error", "eval_time", "scores"]
        
        
        
# determines window size used in EWMA smoothing (percentage of total values for channel)
smoothing_perc: 0.05

# number of values surrounding an error that are brought into the sequence (promotes grouping on nearby sequences
error_buffer: 10



element: "magnitude"
sensor_id: 'R2S1'
monitor: '2500sec'

train: False #Train # train new or existing model for each channel
predict: True #Train # generate new predicts or, if False, use predictions stored locally